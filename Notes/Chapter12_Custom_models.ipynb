{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 20:06:20.788729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # 2x3 tensor of floats\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just like ndarrays, tensors have shape and data type\n",
    "# indexing works just like ndarrays\n",
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]  # equivalent to t[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are many tensor operations\n",
    "t + 10\n",
    "# there are many aliases for operations such as tf.add() and tf.math.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication is done with the matmul() function\n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a tensor can hold a scaler value in this case the shqape is empty\n",
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the keras low lovel api located in td.keras.backend this is usually imported as K for convenience\n",
    "# now that keras is tensor flow we should just use tf low level api directly\n",
    "# tf.reduce_mean is the same as np.mean and tf.reduce_sum is the same as np.sum and so on\n",
    "# tensors work well with numpy you can create a tensor from a numpy array and vice versa and even apply tensor flow operations to numpy arrays\n",
    "a = np.array([2.0, 4.0, 5.0])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)\n",
    "# keep in mind that numpy uses 64 bit precision by default while tensor flow uses 32 bit by default because it runs faster and uses less ram\n",
    "# when you create a tensor from a numpy array make sure to set the dtype argument to 32 bit otherwise tensorflow will create a 64 bit tensor by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor flow does not do any type conversions it just raises an exceeption if you try to execute an operation on tensors with incompatible types\n",
    "# tf.constant(2.) + tf.constant(40)\n",
    "# both of these will raise errors because they are different types\n",
    "# tf.constant(2.) + tf. constant(40, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can use tf.cast() to convert types\n",
    "t2 = tf.constant(40.0, dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tf.tensor values that we have seen so far are immutable meaning that you cannot modify them so we need to use tf.Variable instead\n",
    "v = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a variable acts much like a tensor and you can perform the same operations on it but you can also modify it's value by using the assign() method\n",
    "# this changes the value of the variable in place without creating a new tensor object you can also use the assign_add() or assign_sub() methods\n",
    "v.assign(2 * v)  # v now eauals [[2., 4., 6.], [8., 10., 12.]]\n",
    "v[0, 1].assign(42)  # v now equals [[2., 42., 6.], [8., 10., 12.]]\n",
    "v[:, 2].assign([0.0, 1.0])  # v now equals [[2., 42., 0.], [8., 10., 1.]]\n",
    "v.scatter_nd_update(\n",
    "    indices=[[0, 0], [1, 2]], updates=[100.0, 200.0]\n",
    ")  # v now equals [[100., 42., 0.], [8., 10., 200.]]\n",
    "# direct assignments such as v[1] = [7., 8., 9.] do not work\n",
    "# in practice you will rarely have to create variable manually since keras provides an add_weight() method that will take care of it for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTHER DATA STRUCTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparce tensors are tensors that have a lot of zero values\n",
    "# tensor arrays are lists of tensors that have the same shape and data type\n",
    "# ragged tensors are tensors that have one or more ragged dimensions meaning that along at least one axis some rows have a different number of elements than others\n",
    "# string tensors are regular tensors of type string they are usually created by encoding unicode strings\n",
    "# sets are are represented as regular tensors for example tf.constant([[1, 2], [3, 4]]) represents the set {1, 2} {3, 4} each set is reppresented by a vector in the tensors last axis\n",
    "# queues store tensors across multiple setps FIFO, Priority, Random Shuffle and Padding queues are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing models and training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss functions\n",
    "# training set is noisy so we want to use the huber loss function instead of the mse\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(\n",
    "            30,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            input_shape=input_shape,\n",
    "        ),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 2ms/step - loss: 0.5639 - mae: 0.9240 - val_loss: 0.3238 - val_mae: 0.6426\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2473 - mae: 0.5521 - val_loss: 0.2406 - val_mae: 0.5332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b84c790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"custom_models/my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2031 - mae: 0.4896 - val_loss: 0.1857 - val_mae: 0.4614\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1844 - mae: 0.4626 - val_loss: 0.1765 - val_mae: 0.4501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13be6d3d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we load the custom loss function we need to tell keras how to find it by mapping the function to it's name\n",
    "model = tf.keras.models.load_model(\n",
    "    \"custom_models/my_model_with_a_custom_loss.h5\",\n",
    "    custom_objects={\"huber_fn\": huber_fn},\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1978 - mae: 0.4518 - val_loss: 0.1852 - val_mae: 0.4334\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1921 - mae: 0.4437 - val_loss: 0.2229 - val_mae: 0.4587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13be6da50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    return huber_fn\n",
    "\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"custom_models/my_model_with_a_custom_loss_threshold_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the threshold is not saved when we load the model we need to specify it again when we load the model\n",
    "model = tf.keras.models.load_model(\n",
    "    \"custom_models/my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "    custom_objects={\"huber_fn\": create_huber(2.0)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can solve this by creating a subclass of the keras.losses.Loss class and implementing the get_config() method which keras will call and save the threshold value\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):  # accepts any additional argument\n",
    "        self.threshold = threshold\n",
    "        super().__init__(\n",
    "            **kwargs\n",
    "        )  # handles standard args (e.g., dtype) and keyword args (e.g., name)\n",
    "\n",
    "    def call(\n",
    "        self, y_true, y_pred\n",
    "    ):  # takes the labels and predictions and returns the loss\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(\n",
    "        self,\n",
    "    ):  # returns a dictionary mapping each hyperparameter name to its value it first calls parent class's get_config() method then adds the new hyperparameters to this dictionary\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(\n",
    "            30,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            input_shape=input_shape,\n",
    "        ),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6876 - mae: 0.8894 - val_loss: 0.3309 - val_mae: 0.5998\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2839 - mae: 0.5551 - val_loss: 0.2622 - val_mae: 0.5163\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"custom_models/my_model_with_a_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2289 - mae: 0.4950 - val_loss: 0.2260 - val_mae: 0.4762\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2071 - mae: 0.4692 - val_loss: 0.2082 - val_mae: 0.4589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we load the model we need to tell keras how to find the class by mapping the class to its name\n",
    "model = tf.keras.models.load_model(\n",
    "    \"custom_models/my_model_with_a_custom_loss_class.h5\",\n",
    "    custom_objects={\"HuberLoss\": HuberLoss},\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n",
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most functionality can be customized in a similar way\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2.0 / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0.0, tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these custom functions can be used in the same way as the built-in functions\n",
    "layer = tf.keras.layers.Dense(\n",
    "    1,\n",
    "    activation=my_softplus,\n",
    "    kernel_initializer=my_glorot_initializer,\n",
    "    kernel_regularizer=my_l1_regularizer,\n",
    "    kernel_constraint=my_positive_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a function has hyperparameters that need to be saved along with the model then we need to subclass the appropriate class\n",
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "\n",
    "# we must impliment the call() method for losses and layers and the __call__() method for regularizers, initializers, and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# losses and metrics are not the same thing\n",
    "# losses are used by gradient descent to train a model, they must be differentiable\n",
    "# metrics are used to evaluate a model, they can be non-differentiable\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "# for each batch druing training keras will compute this metric and display the mean of the metric for each epoch\n",
    "precision = tf.keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need to define our own metric we can subclass the keras.metrics.Metric class\n",
    "# this keeps track of the Huber loss and the number of instances seen so far\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(\n",
    "        self, threshold=1.0, **kwargs\n",
    "    ):  # creates the state variables needed by the metric (in this case the total and count) and calls the parent class's constructor\n",
    "        super().__init__(**kwargs)  # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(\n",
    "        self, y_true, y_pred, sample_weight=None\n",
    "    ):  # updates the state variables, given the labels and predictions for one batch (or sample), and optionally the sample weights\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):  # computes and returns the final result\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(\n",
    "        self,\n",
    "    ):  # returns a dictionary mapping each hyperparameter name to its value\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this creates a layer without any weights that will apply the exponential function to the inputs\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "exponential_layer([-1.0, 0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build a custom stateful layer we need to subclass the keras.layers.Layer class\n",
    "class MyDense(\n",
    "    tf.keras.layers.Layer\n",
    "):  # this layer will compute w * x + b where w is a matrix created by the layer, x is the input matrix, and b is a vector created by the layer\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):  # creates the layer's variables (weights)\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"he_normal\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(batch_input_shape)  # must be at the end\n",
    "\n",
    "    def call(self, X):  # performs the computations (in this case w * x + b)\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(\n",
    "        self,\n",
    "    ):  # returns a dictionary mapping each hyperparameter name to its value\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"units\": self.units,\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation),\n",
    "        }\n",
    "\n",
    "\n",
    "# this can now be used like any other layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create a layer with multiple inputs the argument to the call() method must be a tuple containing all the inputs\n",
    "# this example takes two inputs and returns three outputs\n",
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape, \" X2.shape: \", X2.shape)\n",
    "        return X1 + X2, X1 * X2, X1 / X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape1, batch_input_shape1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your layer needs to have different behavior during training and testing then you must add a training argument to the call() method and use it to decide what to do\n",
    "# this adds Gaussian noise during training (for regularization) but not during testing\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(\n",
    "                n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "\n",
    "\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(\n",
    "            30, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "losses and metrics based on model internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the custom losses and metrics we created earlier were all based on the labels and predictions for a single batch (or sample)\n",
    "# this creates a custom MLP model composed of a stack of five hidden layers plus one output layer\n",
    "# the loss associated with this auxiliary output will be called \"reconstruction_loss\" and will only be added to the main loss when trainin\n",
    "# we will encourage the model to preserve as much infromation as possible through the hidden layers this sometimes will improve generalization\n",
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Initializes the ReconstructingRegressor.\n",
    "\n",
    "    Args:\n",
    "        output_dim (int): The number of units in the output layer.\n",
    "        **kwargs: Arbitrary keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, output_dim, **kwargs\n",
    "    ):  # creates the DNN with five hidden layers and one output layer and a Mean streaming metric to track the reconstruction error\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(\n",
    "        self, batch_input_shape\n",
    "    ):  # creates an extra dense layer to reconstruct the inputs created here because the number of inputs is not known until the model is built\n",
    "        \"\"\"\n",
    "        Creates a layer to reconstruct the inputs.\n",
    "\n",
    "        Args:\n",
    "            batch_input_shape (TensorShape or tuple): The shape of the input data.\n",
    "        \"\"\"\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        self.built = True  # WORKAROUND for super().build(batch_input_shape)\n",
    "\n",
    "    def call(\n",
    "        self, inputs, training=None\n",
    "    ):  # processes the inputs through the five hidden layers then passes the result through the reconstruction layer then computes the mean squared error between the reconstruction and the inputs and adds it to the model's list of losses\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): The input data.\n",
    "            training (bool, optional): Whether the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            The output of the model.\n",
    "        \"\"\"\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 2ms/step - loss: 0.9633 - reconstruction_error: 0.9702\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4288 - reconstruction_error: 0.5279\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3926 - reconstruction_error: 0.3947\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3712 - reconstruction_error: 0.3240\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3502 - reconstruction_error: 0.2638\n",
      "162/162 [==============================] - 0s 680us/step\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing gradients using autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1**2 + 2 * w1 * w2\n",
    "\n",
    "\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compute the gradients automatically you can use the tf.GradientTape class\n",
    "w1, w2 = tf.Variable(5.0), tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients\n",
    "# in order to save memory only put the strict minimum inside the tf.GradientTape block or pause recording gradients using tape.stop_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tape is automatically erased after you call its gradient() method so you will get an exception if you try to call gradient() twice\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "# dz_dw2 = tape.gradient(z, w2) # this would raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# if you need to call gradient more than once you must make the tape persistent and delete it each time that you are done with it to release resources\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "print(dz_dw1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "print(dz_dw2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the tape will only track operations involving variables so if you try to compute the gradient of z with regard to anything else it will return None\n",
    "c1, c2 = tf.constant(5.0), tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients\n",
    "# this can be usefull if you want to impliment a regularization loss that penalizes activations that vary a lot when the inputs vary little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to compute the gradients of a vector you have to call the Jacobian() method to get a matrix containing all the partial derivatives\n",
    "# in some cases you may want to stop gradients from backpropagating through some part of your neural network this is usefull for freezing lower layers when training a new DNN on top\n",
    "def f(w1, w2):\n",
    "    return 3 * w1**2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  # the forward pass is not affected by the stop_gradient() function\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(1e-50)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(\n",
    "    z, [x]\n",
    ")  # causes an infiate number because the derivative of sqrt(x) is 1 / (2 * sqrt(x)) and when x = 0 this is infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):  # this makes this numericaly stable\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0.0, z)\n",
    "\n",
    "\n",
    "# in some cases where you may still have numerically unstable gradients so you will have to tell TensorFlow to compute gradients using a more stable function\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    def my_softplus_gradients(grads):\n",
    "        return grads * (1 - 1 / (1 + tf.exp(z)))\n",
    "\n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0.0, z)\n",
    "    return result, my_softplus_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in some cases the fit method is not enough for what we want to do so we can create custom training loops\n",
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(\n",
    "            30,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=l2_reg,\n",
    "        ),\n",
    "        tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# we create a function that will randomly sample a batch of instances from the training set\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "# we next define a function that will display the training status including the number of steps the total number of steps the mean loss since the start of the epoch and other metrics\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join(\n",
    "        [f\"{m.name}: {m.result():.4f}\" for m in [loss] + (metrics or [])]\n",
    "    )\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define some hyperparameters and choose the optimizer, the loss function, and the metrics to monitor\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "362/362 - mean_loss: 2.8571 - mean_absolute_error: 0.6311\n",
      "Epoch 2/5\n",
      "362/362 - mean_loss: 1.5686 - mean_absolute_error: 0.5224\n",
      "Epoch 3/5\n",
      "362/362 - mean_loss: 1.0540 - mean_absolute_error: 0.4973\n",
      "Epoch 4/5\n",
      "362/362 - mean_loss: 0.8184 - mean_absolute_error: 0.4956\n",
      "Epoch 5/5\n",
      "362/362 - mean_loss: 0.7071 - mean_absolute_error: 0.4920\n"
     ]
    }
   ],
   "source": [
    "# below is the custom loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:  # we make a prediction for one batch using the model as a function and we compute the loss\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(\n",
    "                loss_fn(y_batch, y_pred)\n",
    "            )  # we compute the mean over the batch\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "        gradients = tape.gradient(\n",
    "            loss, model.trainable_variables\n",
    "        )  # we compute the gradients of the loss with regard to each trainable variable not all variables\n",
    "        optimizer.apply_gradients(\n",
    "            zip(gradients, model.trainable_variables)\n",
    "        )  # this then performs a Gradient Descent step\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(\n",
    "                    variable.constraint(variable)\n",
    "                )  # we apply any constraints wether they are bias constraints or kernel constraints\n",
    "\n",
    "        mean_loss(loss)  # we update the mean loss and the metrics\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)  # display the status bar\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3f887e270e4c8b93bacd70920449bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0afb1b67e745baa7877442aecaeaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f127f78eb25402f87c5b9d1f639385b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd8838682434ff6ba01acdaf409cc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2f212ebb44437ca6d01a9003e74b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeeaa66614874194b9a3a1e5f22fce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extra code – shows how to use the tqdm package to display nice progress bars\n",
    "\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "\n",
    "                steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x13e70da50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this allows us to use the function as if it was a regular python function but it will actually be executed as a TensorFlow graph\n",
    "# we can also use the tf.function decorator\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "# the original python function is still available via the python_function attribute\n",
    "tf_cube.python_function(2)\n",
    "# the tf.function decorator will automatically create a new graph and will reuse it for subsequent calls it will also automatically handle the creation of variables and the execution of the function\n",
    "# it prunes unused nodes after each call and it will automatically reduce the number of operations by optimizing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you set jit_compile=True when calling tf.function it will use XLA to compile the function into a much faster TensorFlow graph (XLA is a domain-specific compiler for linear algebra that optimizes TensorFlow computations)\n",
    "# tf generates graphs by analyzing the source code of the function so it will only capture for loops if they use static bounds and it will not capture while loops or if statements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
